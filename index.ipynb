{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Churning Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries for plotting and visualisation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\USER\\\\phase3\\\\phase3\\\\phase3\\\\Data Sets\\\\bigml_59c28831336c6604c800002a.csv\")\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of the dataset structure and characteristics\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the structure of the data set\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the statistical summary of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the dataset variables\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Unique Values for each variable.\n",
    "unique_values_per_col = data.nunique()\n",
    "print(\"Number of Unique Values per Column:\")\n",
    "print(unique_values_per_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values in the dataset\n",
    "for column in data.columns:\n",
    "    unique_values = data[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the entire the dataset\n",
    "missing_values_total = data.isna().sum()\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values_per_column = data.isna().sum(axis=0)\n",
    "\n",
    "# Check for missing values in each row\n",
    "missing_values_per_row = data.isna().sum(axis=1)\n",
    "\n",
    "print(\"Missing values in the entire dataset:\")\n",
    "print(missing_values_total)\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values_per_column)\n",
    "\n",
    "print(\"\\nMissing values in each row:\")\n",
    "print(missing_values_per_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  distribution of variables in the dataset\n",
    "# Columns in the dataset\n",
    "numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Plot histograms for numerical variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, column in enumerate(numerical_columns, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.histplot(data[column], kde=True, color='skyblue', bins=20)\n",
    "    plt.title(column)\n",
    "    plt.xlabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=[\"state\", \"phone number\"])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for numerical variables vs. churn\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, column in enumerate(numerical_columns, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.scatterplot(x=column, y='churn', data=data, color='skyblue', alpha=0.5)\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.get_dummies(data, columns=[ \"international plan\", \"voice mail plan\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical to numerical data\n",
    "\n",
    "Using data label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use labe encoding to transform categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data['international plan'] = label_encoder.fit_transform(data['international plan'])\n",
    "\n",
    "# Label encoding 'voice mail plan'\n",
    "data['voice mail plan'] = label_encoder.fit_transform(data['voice mail plan'])\n",
    "\n",
    "# decode international plan\n",
    "decoded_international_plan = label_encoder.inverse_transform(data['international plan'])\n",
    "\n",
    "# decode voice mail plan\n",
    "decoded_voice_mail_plan = label_encoder.inverse_transform(data['voice mail plan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of the dataset\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap to visualise the correlation of the variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = data.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot: Total Day Minutes vs. Total Day Charge with hue of churn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='total day minutes', y='total day charge', hue='churn', data=data)\n",
    "plt.title('Total Day Minutes vs. Total Day Charge (Colored by Churn)')\n",
    "plt.xlabel('Total Day Minutes')\n",
    "plt.ylabel('Total Day Charge')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Total Eve Minutes vs. Total Eve Charge with hue of churn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='total eve minutes', y='total eve charge', hue='churn', data=data)\n",
    "plt.title('Total Eve Minutes vs. Total Eve Charge (Colored by Churn)')\n",
    "plt.xlabel('Total Eve Minutes')\n",
    "plt.ylabel('Total Eve Charge')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Total Night Minutes vs. Total Night Charge with hue of churn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='total night minutes', y='total night charge', hue='churn', data=data)\n",
    "plt.title('Total Night Minutes vs. Total Night Charge (Colored by Churn)')\n",
    "plt.xlabel('Total Night Minutes')\n",
    "plt.ylabel('Total Night Charge')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Total Intl Minutes vs. Total Intl Charge with hue of churn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='total intl minutes', y='total intl charge', hue='churn', data=data)\n",
    "plt.title('Total Intl Minutes vs. Total Intl Charge (Colored by Churn)')\n",
    "plt.xlabel('Total Intl Minutes')\n",
    "plt.ylabel('Total Intl Charge')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Customer Service Calls vs. Churn with hue of churn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='customer service calls', y='churn', hue='churn', data=data)\n",
    "plt.title('Customer Service Calls vs. Churn (Colored by Churn)')\n",
    "plt.xlabel('Customer Service Calls')\n",
    "plt.ylabel('Churn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Visualisation \n",
    "\n",
    "Check the distriution of variables with high correlation with churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of total day charge and churn status\n",
    "fig = px.histogram(data, x=\"total day charge\", color=\"churn\", \n",
    "                   title=\"Distribution of Total day Charge by Churn Status\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of total int charge verse vi churn\n",
    "fig = px.histogram(data, x=\"total intl charge\", color=\"churn\", \n",
    "                   title=\"Distribution of Total Intl Charge by Churn Status\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of customer service calls and churn\n",
    "fig = px.histogram(data, x=\"customer service calls\", color=\"churn\",\n",
    "                   title=\"Distribution of customer service calls by Churn Status\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of total eve change and churn\n",
    "fig = px.histogram(data, x=\"total eve charge\", color=\"churn\",\n",
    "                   title=\"Distribution of total eve charge by Churn Status\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of international plan and churn\n",
    "fig = px.histogram(data, x=\"international plan\", color=\"churn\",\n",
    "                   title=\"Distribution of International Plan with Color Encoding by Churn Status\",\n",
    "                   labels={\"international plan\": \"International Plan\", \"churn\": \"Churn Status\"})\n",
    "fig.update_xaxes(type='category')  # Ensure 'international plan' is treated as a categorical variable\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=[ \"international plan\", \"voice mail plan\", \"churn\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in data.columns:\n",
    "    print(var,'\\n', data[var].value_counts()/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate  continuous and categorical features\n",
    "continuous_features = ['account length', 'total day minutes', 'total day calls', \n",
    "                       'total day charge', 'total eve minutes', 'total eve calls', \n",
    "                       'total eve charge', 'total night minutes', 'total night calls', \n",
    "                       'total night charge', 'total intl minutes', 'total intl calls', \n",
    "                       'total intl charge', 'customer service calls']\n",
    "\n",
    "categorical_features = ['area code', 'number vmail messages', 'international plan_1', 'voice mail plan_1']\n",
    "X_continuous = data[continuous_features]\n",
    "X_categorical = data[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, train and test sets\n",
    "X_train_cont, X_test_cont, X_train_cat, X_test_cat, y_train, y_test = train_test_split(X_continuous, X_categorical, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocess continous and categorical features\n",
    "continuous_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cont', continuous_transformer, continuous_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "rf_classifier = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1648,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1663,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (2666, 18), indices imply (2666, 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1663], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m continuous_features \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Assuming df is your original DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert numpy array to pandas DataFrame with specified column names\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X_train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train, columns\u001b[38;5;241m=\u001b[39mcontinuous_features)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Plot histograms\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m continuous_features:    \n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:785\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    774\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    775\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    776\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    782\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 785\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    786\u001b[0m             data,\n\u001b[0;32m    787\u001b[0m             index,\n\u001b[0;32m    788\u001b[0m             columns,\n\u001b[0;32m    789\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    790\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    791\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    794\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (2666, 18), indices imply (2666, 19)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the column names from the original DataFrame\n",
    "continuous_features = data.columns.tolist()  # Assuming df is your original DataFrame\n",
    "\n",
    "# Convert numpy array to pandas DataFrame with specified column names\n",
    "X_train_df = pd.DataFrame(X_train, columns=continuous_features)\n",
    "\n",
    "# Plot histograms\n",
    "for feat in continuous_features:    \n",
    "    fig = X_train_df[feat].hist(bins=20)\n",
    "    fig.set_ylabel('number of cases')\n",
    "    fig.set_xlabel(feat)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3333, 19)"
      ]
     },
     "execution_count": 1662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target variable\n",
    "X = data.drop(columns=[\"churn\"])\n",
    "y = data[\"churn\"]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target variable to numerical values\n",
    "data['churn'] = data['churn'].astype(int)\n",
    "data['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
